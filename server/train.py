import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from config import DATASET_DIR, MODEL_DIR, NUM_EPOCHS, PATIENCE

def train_model():
    train_dir = os.path.join(DATASET_DIR, "train")
    val_dir   = os.path.join(DATASET_DIR, "val")

    if not os.path.exists(train_dir) or not os.path.exists(val_dir):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ° train/ æˆ– val/ æ–‡ä»¶å¤¹ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {DATASET_DIR}")

    os.makedirs(MODEL_DIR, exist_ok=True)

    # æ•°æ®å¢å¼º
    train_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])
    val_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])

    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
    val_dataset   = datasets.ImageFolder(val_dir, transform=val_transforms)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)

    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)} | éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
    print(f"ğŸ“Š ç±»åˆ«æ•°: {len(train_dataset.classes)}")

    # æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ğŸ’» ä½¿ç”¨è®¾å¤‡:", device)
    model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
    model.classifier[1] = nn.Linear(model.last_channel, len(train_dataset.classes))
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    best_val_loss = float("inf")
    best_epoch = 0
    patience_counter = 0
    stopped_early = False

    for epoch in range(NUM_EPOCHS):
        # ====== è®­ç»ƒ ======
        model.train()
        running_loss, running_corrects = 0.0, 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
        train_loss = running_loss / len(train_dataset)
        train_acc = running_corrects.double() / len(train_dataset)

        # ====== éªŒè¯ ======
        model.eval()
        val_loss, val_corrects = 0.0, 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)
                val_loss += loss.item() * inputs.size(0)
                val_corrects += torch.sum(preds == labels.data)
        val_loss = val_loss / len(val_dataset)
        val_acc = val_corrects.double() / len(val_dataset)

        print(f"Epoch {epoch+1}/{NUM_EPOCHS} "
              f"| Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} "
              f"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

        # ====== ä¿å­˜ & æ—©åœ ======
        if val_loss < best_val_loss:  # âœ… ç”¨ Val Loss åˆ¤æ–­
            best_val_loss = val_loss
            best_epoch = epoch + 1
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            pth_path = os.path.join(MODEL_DIR, "mobilenet_classifier.pth")
            torch.save(model.state_dict(), pth_path)
            print(f"ğŸ’¾ æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Val Loss={best_val_loss:.4f} @ Epoch {best_epoch})")
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print(f"â¹ï¸ éªŒè¯é›† Loss è¿ç»­ {PATIENCE} æ¬¡æœªä¸‹é™ï¼Œæå‰åœæ­¢è®­ç»ƒã€‚")
                stopped_early = True
                break

    # ====== è®­ç»ƒç»“æŸæç¤º ======
    if stopped_early:
        print(f"ğŸ† æœ€ä½³æ¨¡å‹å‡ºç°åœ¨ Epoch {best_epoch} (Val Loss={best_val_loss:.4f})")
    else:
        print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è½®æ•° {NUM_EPOCHS}ï¼Œæœ€ä½³æ¨¡å‹åœ¨ Epoch {best_epoch} (Val Loss={best_val_loss:.4f})")
        print("ğŸ‘‰ å¯ä»¥è€ƒè™‘å¢å¤§ NUM_EPOCHS æˆ–è°ƒæ•´ PATIENCE ä»¥è·å¾—æ›´é«˜ç²¾åº¦ã€‚")

    return model, len(train_dataset.classes)
